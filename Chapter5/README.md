# 决策树
## 笔记摘要
* 决策树可以认为是if-then规则的集合，也可以认为是定义在特征空间上的条件概率分布
* 根据损失函数最小化的原则建立决策树模型
* 决策树的路径或其对应的if-then规则集合具有一个重要性质：互斥且完备
* 决策树的学习算法包含特征选择、决策树的生成与决策树的剪枝
* 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择


### 熵

$$
H(p)=H(X)=-\sum_{i=1}^{n}p_i\log p_i
$$

* **熵只与$X$的分布有关，与$X$取值无关**

* 定义$0\log0=0$，熵是非负的
* 表示随机变量不确定性的度量

### 条件熵

* 随机变量$(X,Y)$的联合概率分布为

$$P(X=x_i,Y=y_j)=p_{ij}, i=1,2,\dots ,n;j=1,2,\dots ,m$$

* 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性
$$
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
$$
其中$p_i=P(X=x_i),i=1,2,\dots ,n$

### 经验熵， 经验条件熵

* 当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵



### 信息增益

* 特征$A$对训练数据集$D$的信息增益$g(D|A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定的条件下$D$的经验条件熵$H(D|A)$之差
$$
g(D,A)=H(D)-H(D|A)
$$

* 熵与条件熵的差称为互信息

* 决策树中的信息增益等价于训练数据集中的类与特征的互信息

* 考虑ID这种特征， 本身是唯一的。按照ID做划分， 得到的经验条件熵为0, 会得到最大的信息增益。所以， 按照信息增益的准则来选择特征， 可能会倾向于取值比较多的特征



### 信息增益比
* 使用信息增益比可以对上面倾向取值较多的特征的问题进行校正
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\
H_A(D)=-\sum_{i=1}^n\frac{D_i}{D}log_2\frac{D_i}{D}
$$

### ID3算法

> 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
> 输出：决策树$T$
>
> 1. 如果$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该节点的类标记，返回$T$
> 1. 如果$A$是空集，则$T$为单节点树，并将实例数最多的类作为该节点类标记，返回$T$
> 1. 计算$g$, 选择信息增益最大的特征$A_g$
> 1. 如果$A_g$的信息增益小于$\epsilon$，则置$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
> 1. 否则，依$A_g=a_i$将D划分若干非空子集$D_i$，$D_i$中实例数最大的类$C_k$作为类标记，构建子结点，由结点及其子结点 构成树$T$，返回$T$
> 1. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$





###  C4.5的生成算法
* 改用信息增益比来选择特征
> 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
> 输出：决策树$T$
>
> 1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
> 1. 如果$A$是空集, 置$T$为单节点树，实例数最多的作为该节点类标记，返回$T$
> 1. 计算$g$, 选择**信息增益比**最大的特征$A_g$
> 1. 如果$A_g$的**信息增益比**小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
> 1. 否则，依$A_g=a_i$将D划分若干非空子集$D_i$，$D_i$中实例数最大的类$C_k$作为类标记，构建子结点，由结点及其子结点 构成树$T$，返回$T$
> 1. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$

*  ID3和C4.5在生成上，差异只在准则的差异



### 树的剪枝
* 决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现的
* 树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵， $\alpha\geqslant 0$为参数，决策树学习的损失函数可以定义为
 $$
 C_\alpha(T)=\sum_{i=1}^{|T|}N_tH_t(T)+\alpha|T|
 $$
 其中
 $$
 H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}
 $$
 $$
 C(T)=\sum_{t=1}^{|T|}N_tH_t(T)\color{black}=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}
 $$
  这时有
 $$
 C_\alpha(T)=C(T)+\alpha|T|
 $$
其中$C(T)$表示模型对训练数据的误差，$|T|$表示模型复杂度，参数$\alpha \geqslant 0$控制两者之间的影响


### 剪枝算法

> 输入：生成算法生成的整个树$T$，参数$\alpha$
>
> 输出：修剪后的子树$T_\alpha$
>
> 1. 计算每个结点的经验熵
> 1. 递归地从树的叶结点向上回缩
>    假设一组叶结点回缩到其父结点之前与之后的整体树分别是$T_B$和$T_A$，其对应的损失函数分别是$C_\alpha(T_A)$和$C_\alpha(T_B)$，如果$C_\alpha(T_A)\leqslant C_\alpha(T_B)$则进行剪枝，即将父结点变为新的叶结点
> 1. 返回2，直至不能继续为止，得到损失函数最小的子树$T_\alpha$
* 决策树的剪枝算法可以由一种动态规划的算法实现

### CART

* 决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，并进行特征选择，生成二叉树
####  最小二乘回归树生成算法
* 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上地输出值，构建二叉决策树

>输入：训练数据集$D$
>输出：回归树$f(x)$
>步骤：
>1. 遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，得到满足下面式子的$(j,s)$
   $$
   \min\limits_{j,s}\left[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]
   $$
>1. 用选定的$(j,s)$, 划分区域并决定相应的输出值
   $$
   R_1(j,s)=\{x|x^{(j)}\leq s\}, R_2(j,s)=\{x|x^{(j)}> s\} \\
   \hat{c}_m= \frac{1}{N}\sum\limits_{x_i\in R_m(j,s)} y_j, x\in R_m, m=1,2
   $$
>1. 对两个子区域调用(1)(2)步骤， 直至满足停止条件
>1. 将输入空间划分为$M$个区域$R_1, R_2,\dots,R_M$，生成决策树：
   $$
   f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)
   $$
   * 课后题有详细例子






####  CART分类树的生成

* 分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点

* 概率分布的基尼指数定义

$$
Gini(p) = \sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$
* 如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，则在特征$A$的条件下，集合$D$的基尼指数定义为
$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$ 
>输入：训练数据集$D$，停止计算的条件
>输出：CART决策树
>根据训练数据集，从根节点开始，递归地对每个结点进行以下操作，构建二叉决策树：
>1. 设结点地训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征$A$，对其可能取得每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分成$D_1$和$D_2$两部分，计算$A=a$时的基尼指数
>1. 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依照最优特征和最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。
>1. 对两个子结点递归地调用1、2，直至满足停止条件
>1. 生成CART决策树
* 算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于指定阈值，或者没有更多特征


## 习题解答

* 5.1 根据表 5.1 所给的训练数据集，利用信息增益比（C4.5 算法）生成决策树
  * 编写程序计算信息增益比，并利用C4.5算法生成决策树，得到结果如下
特征（年龄）的信息增益比为： 0.052
特征（有工作）的信息增益比为： 0.352
特征（有自己的房子）的信息增益比为： 0.433
特征（信贷情况）的信息增益比为： 0.232
特征（年龄）的信息增益比为： 0.164
特征（有工作）的信息增益比为： 1.000
特征（有自己的房子）的信息增益比为： 0.340
   * 决策树
{'label:': None, 'feature': 2, 'tree': {'否': {'label:': None, 'feature': 1, 'tree': {'否': {'label:': '否', 'feature': None, 'tree': {}}, '是': {'label:': '是', 'feature': None, 'tree': {}}}}, '是': {'label:': '是', 'feature': None, 'tree': {}}}}
  * 结果与ID3算法完全相同
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/2019060915344250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5YzQ0ODEzNDE4,size_16,color_FFFFFF,t_70#pic_center)  

* 5.2 已知如表 5.2 所示的训练数据，试用平方误差损失准则生成一个二叉回归树
![在这里插入图片描述](https://img-blog.csdnimg.cn/201906091538427.png#pic_center)

* 5.3 证明 CART 剪枝算法中，当$α$确定时，存在唯一的最小子树 $T_α$使损失函数 $C_α(T)$最小
   * 利用反证法，假设存在两个最小子树使损失函数最小 
    设这两棵最小子树 为$T_1$​ 和 $T_2$​ ，其剪枝位置分别是 $t_1$​ 和 $t_2$​ ，两者都能使得损失函数最小，即两者拥有相等的 $C_\alpha(T)​$ 
    又有：
$$C_\alpha(t_1)<C_\alpha(T_{t1})\\ C_\alpha(t_2)<C_\alpha(T_{t2})$$ 
即剪枝$t_1$,$t_2$ ，总能使得整体损失函数减小，因此对于子树 $T_1$, $T_2$ ，总存在进一步的剪枝，使得损失函数进一步减小（在 $T_1$ 中剪枝 $t_2$,在 $T_2$ 中剪枝 $t_1$ ），因此  $T_1$, $T_2$ 不是最优的子树。
即不可能存在两棵及以上的最优子树

* 5.4 证明 CART 剪枝算法中求出的子树序列$\{T_0,T_1,⋅⋅⋅,T_n\}$分别是区间 $α∈[α_i,α_{i+1})$的最优子树$T_\alpha$ ，这里$i=0,1,⋅⋅⋅,n,0=\alpha_0<\alpha_1<\cdot\cdot\cdot<\alpha_n<+\infty$
  * 参见 [Breiman的著作](https://www.taylorfrancis.com/books/9781315139470)