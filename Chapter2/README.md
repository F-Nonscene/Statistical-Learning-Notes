# 感知机
## 学习笔记
* 感知机是二类分类的线性模型,属于**判别模型**
* 感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础
* 损失函数选择
     * 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数不是参数$w,b$的连续可导函数，不易优化
     *  损失函数的另一个选择是误分类点到超平面$S$的总距离，这正是感知机所采用的

* 感知机学习的经验风险函数(损失函数)
$$
L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)
$$
其中$M$是误分类点的集合，给定训练数据集$T$，损失函数$L(w,b)$是$w$和$b$的连续可导函数

#### 原始形式算法

> 输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in R^n,y_i\in {\{+1,-1\}},i=1,2,3,\dots,N;学习率0<\eta\leqslant 1$
>
> 输出：$w,b;感知机模型f(x)=sign(w\cdot x+b)$
>
> 1. 选取初值$w_0,b_0$
>
> 1. 训练集中选取数据$(x_i,y_i)$
>
> 1. 如果$y_i(w\cdot x_i+b)\leqslant 0$
>    $$
>    w\leftarrow w+\eta y_ix_i \\
>    b\leftarrow b+\eta y_i
>    $$
> 4. 转至(2)，直至训练集中没有误分类点

* 这个是原始形式中的迭代公式，可以对$x​$补1，将$w​$和$b​$合并在一起，合在一起的这个叫做扩充权重向量

#### 对偶形式算法

* 对偶形式的基本思想是将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$。



> 输入：$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in R^n,y_i\in {\{+1,-1\}},i=1,2,3,\dots,N;学习率0<\eta\leqslant 1$
>
> 输出：
$\alpha ,b; 感知机模型f(x)=sign\left(\sum_{j=1}^N\alpha_jy_jx_j\cdot x+b\right),
\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^T$
>
> 1. $\alpha \leftarrow 0,b\leftarrow 0​$
>
> 1. 训练集中选取数据$(x_i,y_i)$
>
> 1. 如果$y_i\left(\sum_{j=1}^N\alpha_jy_jx_j\cdot x+b\right) \leqslant 0​$
> $$
> \alpha_i\leftarrow \alpha_i+\eta \\
> b\leftarrow b+\eta y_i
> $$
>
> 4. 转至(2)，直至训练集中没有误分类点

* **Gram matrix**

   对偶形式中，训练实例仅以内积的形式出现。

   为了方便可预先将训练集中的实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵
   $$
G=[x_i\cdot x_j]_{N\times N} 
$$




## 习题解答
* 2.1 Minsky 与 Papert 指出：感知机因为是线性模型，所以不能表示复杂的函数，如异或 (XOR)。验证感知机为什么不能表示异或。
    * 我们显然无法使用一条直线将两类样本划分，异或问题是线性不可分的。
    * 可以借助下面动图理解


![在这里插入图片描述](https://img-blog.csdnimg.cn/20190527153433865.gif#pic_center)

* 2.2 模仿例题 2.1，构建从训练数据求解感知机模型的例子。
    * 感知机的训练过程如上所述，取与原例题相同的数据，计算出不同的结果
    * x = [3 3; 4 3; 1 1];
y = [1; 1; -1]; 
    * 根据程序运行可得：
    * eg1:
    误分类点为： [3 3] 此时的w和b为： [[0.][0.]]  0
误分类点为： [1 1] 此时的w和b为： [[3.][3.]]  1
误分类点为： [1 1] 此时的w和b为： [[2.][2.]]  0
误分类点为： [1 1] 此时的w和b为： [[1.][1.]]  -1
误分类点为： [3 3] 此时的w和b为： [[0.][0.]]  -2
误分类点为： [1 1] 此时的w和b为： [[3.][3.]] -1
误分类点为： [1 1] 此时的w和b为： [[2. [2.]] -2
最终训练得到的w和b为： [[1.] [1.]] -3 
即$f(x)=sign(x^{(1)}+x^{(2)}-3)$
  * eg2:
  误分类点为： [1 1] 此时的w和b为： [[0.] [0.]] 0
误分类点为： [4 3] 此时的w和b为： [[-1.] [-1.]] -1
误分类点为： [1 1] 此时的w和b为： [[3.] [2.]] 0
误分类点为： [1 1] 此时的w和b为： [[2.] [1.]] -1
最终训练得到的w和b为： [[1.] [0.]] -2
即$f(x)=sign(x^{(1)}-2)$
